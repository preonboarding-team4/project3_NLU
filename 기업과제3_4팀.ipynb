{"cells":[{"cell_type":"markdown","id":"f5eae3b4","metadata":{"id":"f5eae3b4"},"source":["사용한 모델: KR-BERT_character_sub-character  \n","모델 repo: https://github.com/snunlp/KR-BERT"]},{"cell_type":"markdown","id":"ad044818","metadata":{"id":"ad044818"},"source":["# 0. 환경설정"]},{"cell_type":"code","execution_count":null,"id":"ecf5268f","metadata":{"id":"ecf5268f"},"outputs":[],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","from torch.nn.utils import clip_grad_norm_\n","from sklearn.model_selection import train_test_split\n","from torch.optim import AdamW, NAdam\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import unicodedata\n","import re\n","from pathlib import Path\n","from typing import Union\n","from transformers import BertModel, BertForPreTraining, BertTokenizer, BertPreTrainedModel, BertConfig\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import f1_score\n","from scipy.stats import pearsonr\n","from transformers import get_linear_schedule_with_warmup, get_constant_schedule, get_cosine_with_hard_restarts_schedule_with_warmup\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":null,"id":"d8f05a86","metadata":{"scrolled":true,"id":"d8f05a86","outputId":"7ed075a7-d100-439d-98cc-b7d329930722"},"outputs":[{"name":"stdout","output_type":"stream","text":["# available GPUs : 1\n","GPU name : GeForce RTX 3070\n","cuda\n"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n","    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n","else:\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","id":"4d6572be","metadata":{"id":"4d6572be"},"source":["# 1. 데이터"]},{"cell_type":"code","execution_count":null,"id":"1e64498b","metadata":{"id":"1e64498b"},"outputs":[],"source":["class Config:\n","    \"\"\"Config class\"\"\"\n","\n","    def __init__(self, json_path_or_dict: Union[str, dict]) -> None:\n","        \"\"\"Instantiating Config class\n","        Args:\n","            json_path_or_dict (Union[str, dict]): filepath of config or dictionary which has attributes\n","        \"\"\"\n","        if isinstance(json_path_or_dict, dict):\n","            self.__dict__.update(json_path_or_dict)\n","        else:\n","            with open(json_path_or_dict, mode=\"r\") as io:\n","                params = json.loads(io.read())\n","            self.__dict__.update(params)\n","\n","    def save(self, json_path: Union[str, Path]) -> None:\n","        \"\"\"Saving config to json_path\n","        Args:\n","            json_path (Union[str, Path]): filepath of config\n","        \"\"\"\n","        with open(json_path, mode=\"w\") as io:\n","            json.dump(self.__dict__, io, indent=4)\n","\n","    def update(self, json_path_or_dict) -> None:\n","        \"\"\"Updating Config instance\n","        Args:\n","            json_path_or_dict (Union[str, dict]): filepath of config or dictionary which has attributes\n","        \"\"\"\n","        if isinstance(json_path_or_dict, dict):\n","            self.__dict__.update(json_path_or_dict)\n","        else:\n","            with open(json_path_or_dict, mode=\"r\") as io:\n","                params = json.loads(io.read())\n","            self.__dict__.update(params)\n","\n","    @property\n","    def dict(self) -> dict:\n","        return self.__dict__\n","\n","\n","def data_preproc(paragrahp:str):\n","    \"\"\"\n","    1. 괄호 및 괄호 안 글자 제거\n","    2. 글자 인코딩 변경\n","    3. 홈페이지 주소 제거\n","    4. 이메일 주소 제거\n","    \"\"\"\n","    paragrahp = re.sub(r'\\(.*\\)', '', paragrahp)\n","    patten = r\"[^ .,·?!:'”%/()A-Za-z0-9가-힣+]\"\n","    paragrahp = re.sub(patten, \" \", paragrahp)\n","    paragrahp = \" \".join(paragrahp.split())\n","    paragrahp = unicodedata.normalize(\"NFKD\", paragrahp)\n","    paragrahp = re.sub(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", \"\", paragrahp)\n","    paragrahp = re.sub(\"'^[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\", \"\", paragrahp)\n","    return paragrahp"]},{"cell_type":"markdown","id":"35613bfa","metadata":{"id":"35613bfa"},"source":["# Data augmentation"]},{"cell_type":"markdown","id":"12d2fe95","metadata":{"id":"12d2fe95"},"source":["아래의 마크다운으로 처리된 코드들은 데이터 증강을 위한 코드입니다. 총 3가지 방법으로 데이터를 증강하였습니다. 각각의 방법으로 증강한 데이터를 원본데이터와 함께 사용하여 학습을 시도하였습니다. back translation 방법을 사용하여 데이터를 증강한 방법이 3가지 방법 중 가장 좋은 성능을 보였습니다. 하지만, 원본데이터만 사용한 것과 비교하여 일관되게 성능이 하락하는 양상을 보였고, 최종적으로 원본데이터만 사용하는 것으로 결정하였습니다."]},{"cell_type":"markdown","id":"58b6736d","metadata":{"id":"58b6736d"},"source":["```python\n","import random\n","from googletrans import Translator\n","\n","# KorEDA\n","# https://github.com/catSirup/KorEDA/tree/master\n","# 기반 논문: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\n","\n","def random_deletion(words, p):\n","    if len(words) == 1:\n","        return words\n","\n","    new_words = []\n","    for word in words:\n","        r = random.uniform(0, 1)\n","        if r > p:\n","            new_words.append(word)\n","\n","    if len(new_words) == 0:\n","        rand_int = random.randint(0, len(words)-1)\n","        return [words[rand_int]]\n","\n","    return new_words\n","\n","def random_swap(words, n):\n","    new_words = words.copy()\n","    for _ in range(n):\n","        new_words = swap_word(new_words)\n","\n","    return new_words\n","\n","def swap_word(new_words):\n","    random_idx_1 = random.randint(0, len(new_words)-1)\n","    random_idx_2 = random_idx_1\n","    counter = 0\n","\n","    while random_idx_2 == random_idx_1:\n","        random_idx_2 = random.randint(0, len(new_words)-1)\n","        counter += 1\n","        if counter > 3:\n","            return new_words\n","\n","    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n","    return new_words\n","\n","def EDA(sentence, alpha_rs=0.1, p_rd=0.1, num_aug=1):\n","    words = sentence.split(' ')\n","    words = [word for word in words if word is not \"\"]\n","    num_words = len(words)\n","\n","    num_new_per_technique = int(num_aug/4) + 1\n","\n","    n_rs = max(1, int(alpha_rs*num_words))\n","\n","    # rs\n","    for _ in range(num_new_per_technique):\n","        a_words = random_swap(words, n_rs)\n","        rs_result = \" \".join(a_words)\n","\n","    # rd\n","    for _ in range(num_new_per_technique):\n","        a_words = random_deletion(words, p_rd)\n","        rd_result = \" \".join(a_words)\n","\n","    return rs_result, rd_result\n","```"]},{"cell_type":"markdown","id":"16a8a780","metadata":{"id":"16a8a780"},"source":["```python\n","# Back translattion\n","# 참고 논문: Data expansion using back translation and paraphrasing for hate speech detection\n","\n","def back_translattion_using_google(text):\n","    translator = Translator()\n","    result = translator.translate(text, src=\"ko\", dest=\"en\")\n","    result = translator.translate(result.text, src=\"en\", dest=\"ko\")\n","\n","    return result.text\n","```"]},{"cell_type":"markdown","id":"804b2b7f","metadata":{"id":"804b2b7f"},"source":["```python\n","def data_augument(df):\n","    sent_1_list=df.sentence1.to_list()\n","    sent_2_list=df.sentence2.to_list()\n","    label_list=df.label.to_list()\n","\n","    random_swap_result = list()\n","    random_delete_result = list()\n","    translate_result = list()\n","\n","    for idx, sent1 in enumerate(sent_1_list):\n","        sent2 = sent_2_list[idx]\n","        s1_rs, s1_rd = EDA(sent1)\n","        s2_rs, s2_rd = EDA(sent2)\n","\n","        random_swap_result.append([s1_rs, s2_rs, label_list[idx]])\n","        random_delete_result.append([s1_rd, s2_rd, label_list[idx]])\n","\n","        try:\n","            translate_result.append([back_translattion_using_google(sent1), back_translattion_using_google(sent2), label_list[idx]])\n","        except:\n","            translate_result=list()\n","    \n","    random_swap_df = pd.DataFrame (random_swap_result, columns = ['sentence1', 'sentence2', 'label'])\n","    random_delete_df = pd.DataFrame (random_delete_result, columns = ['sentence1', 'sentence2', 'label'])\n","    back_translate_df = pd.DataFrame (translate_result, columns = ['sentence1', 'sentence2', 'label'])\n","    return random_swap_df, random_delete_df, back_translate_df\n","\n","rs, rd, bt = data_augument(temp)\n","```"]},{"cell_type":"markdown","id":"c64a1f64","metadata":{"id":"c64a1f64"},"source":["---"]},{"cell_type":"markdown","id":"ccea75de","metadata":{"id":"ccea75de"},"source":["# 2. 모델링"]},{"cell_type":"code","execution_count":null,"id":"ac66ea9f","metadata":{"id":"ac66ea9f"},"outputs":[],"source":["with open(\"./klue-sts-data/klue-sts-v1.1_train.json\", \"rt\", encoding='utf8') as f:\n","    data = json.load(f)"]},{"cell_type":"code","execution_count":null,"id":"4ea94ebe","metadata":{"id":"4ea94ebe"},"outputs":[],"source":["def custom_collate_fn(batch):\n","    input_list, target_list = [], []\n","    \n","    for _input, _target in batch:\n","        input_list.append(_input)\n","        target_list.append(_target)\n","    \n","    tensorized_input = tokenizer_krbert_sub(\n","        input_list,\n","        add_special_tokens=True,\n","        padding=\"longest\",\n","        truncation=True,\n","        max_length=128,\n","        return_tensors='pt'\n","    )\n","    \n","    tensorized_label = torch.tensor(target_list, dtype = torch.float)\n","    \n","    return tensorized_input, tensorized_label\n","\n","\n","class CustomDataset(Dataset):\n","    \"\"\"\n","    - input_data: list of string\n","    - target_data: list of int\n","    \"\"\"\n","    def __init__(self, input_data:list, target_data:list) -> None:\n","        self.X = input_data\n","        self.Y = target_data\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        X = self.X[index]\n","        Y = self.Y[index]\n","        return X, Y"]},{"cell_type":"code","source":["class FCLayer(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=None):\n","        super(FCLayer, self).__init__()\n","        self.use_activation = use_activation\n","        self.dropout = torch.nn.Dropout(dropout_rate)\n","        self.linear = torch.nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        if self.use_activation:\n","            x = self.use_activation(x)\n","        return self.linear(x)\n","\n","\n","class BertSts(BertPreTrainedModel):\n","    def __init__(self, config) -> None:\n","        super(BertSts, self).__init__(config)\n","        self.bert = BertModel(config)\n","        self.layer = FCLayer(config.hidden_size, \n","                             config.hidden_size, \n","                             config.hidden_dropout_prob, \n","                             torch.nn.GELU())\n","        self.Dense = torch.nn.Sequential(self.layer)\n","        self.output_layer = FCLayer(config.hidden_size, 1)\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        dense_outputs = self.Dense(bert_outputs['pooler_output'])\n","        sim_score = self.output_layer(dense_outputs)\n","        return sim_score"],"metadata":{"id":"BVV5YJgCbSUv"},"id":"BVV5YJgCbSUv","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8ae387a6","metadata":{"id":"8ae387a6"},"source":["---"]},{"cell_type":"markdown","id":"c990c42c","metadata":{"id":"c990c42c"},"source":["### 데이터 전처리 및 pandas dataframe으로 변경"]},{"cell_type":"code","execution_count":null,"id":"7e24118a","metadata":{"id":"7e24118a"},"outputs":[],"source":["shape = np.full([len(data), 3], np.nan)\n","df = pd.DataFrame(shape, columns=['sentence1', 'sentence2', 'label'])"]},{"cell_type":"code","execution_count":null,"id":"7e7d92da","metadata":{"id":"7e7d92da"},"outputs":[],"source":["for idx, el in enumerate(data):\n","    df.loc[idx] = [el['sentence1'], el['sentence2'], el['labels']['real-label']]"]},{"cell_type":"code","execution_count":null,"id":"fbb7a360","metadata":{"scrolled":true,"id":"fbb7a360","outputId":"830a1575-7f29-4ace-fdd5-edd5e74e8ae3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>사례집은 국립환경과학원 누리집(ecolibrary.me.go.kr)에서 12일부터 ...</td>\n","      <td>주말을 제외한 평일 오후 12시 30분부터 문예회관 공식 페이스북과 유튜브에서는 지...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           sentence1  \\\n","7  사례집은 국립환경과학원 누리집(ecolibrary.me.go.kr)에서 12일부터 ...   \n","\n","                                           sentence2  label  \n","7  주말을 제외한 평일 오후 12시 30분부터 문예회관 공식 페이스북과 유튜브에서는 지...    0.0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df.loc[7:7]"]},{"cell_type":"code","execution_count":null,"id":"017ef7d8","metadata":{"id":"017ef7d8"},"outputs":[],"source":["df[['sentence1', 'sentence2']] = df[['sentence1', 'sentence2']].applymap(data_preproc)"]},{"cell_type":"code","execution_count":null,"id":"cda9a376","metadata":{"scrolled":true,"id":"cda9a376","outputId":"beb81d45-b723-4002-e503-bf16a2e7c7a0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>사례집은 국립환경과학원 누리집에서 12...</td>\n","      <td>주말을 제외한 평일 오후 12시 30분부터 무...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           sentence1  \\\n","7  사례집은 국립환경과학원 누리집에서 12...   \n","\n","                                           sentence2  label  \n","7  주말을 제외한 평일 오후 12시 30분부터 무...    0.0  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df.loc[7:7]"]},{"cell_type":"code","execution_count":null,"id":"2366a501","metadata":{"id":"2366a501"},"outputs":[],"source":["train, valid = train_test_split(df, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"db183d7d","metadata":{"id":"db183d7d"},"outputs":[],"source":["del data"]},{"cell_type":"markdown","id":"54ec634e","metadata":{"id":"54ec634e"},"source":["### read config files"]},{"cell_type":"code","execution_count":null,"id":"5fd18e7d","metadata":{"id":"5fd18e7d"},"outputs":[],"source":["conf_info = Config(f\"./config_files/config_subchar12367_bert.json\")"]},{"cell_type":"code","execution_count":null,"id":"a2ffdf9b","metadata":{"scrolled":true,"id":"a2ffdf9b"},"outputs":[],"source":["with open(\"./config_files/config_subchar12367_bert.json\", \"rt\", encoding=\"utf8\") as f:\n","    conf_subchar = json.load(f)"]},{"cell_type":"code","execution_count":null,"id":"1bcfd6bd","metadata":{"scrolled":true,"id":"1bcfd6bd","outputId":"16e68b83-1c28-4841-fdee-602aba4be870"},"outputs":[{"data":{"text/plain":["{'config': 'config_files/bert_config_subchar12367.json',\n"," 'bert': 'torch_model/pytorch_model_subchar12367_bert.bin',\n"," 'tokenizer': 'config_files/vocab_snu_subchar12367.txt',\n"," 'vocab': 'config_files/vocab_snu_subchar12367.pkl'}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["conf_subchar"]},{"cell_type":"code","execution_count":null,"id":"e060c1ea","metadata":{"id":"e060c1ea","outputId":"b1719274-a296-4fd1-865f-5ac1a8c4ac38"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HwaLang\\miniconda3\\envs\\nnitest\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n","  warnings.warn(\n"]}],"source":["tokenizer_krbert_sub = BertTokenizer.from_pretrained(f\"{conf_info.tokenizer}\")"]},{"cell_type":"code","execution_count":null,"id":"7c4cfa47","metadata":{"scrolled":true,"id":"7c4cfa47","outputId":"4019691c-525f-4aed-f101-8e7b53137f9d"},"outputs":[{"data":{"text/plain":["'torch_model/pytorch_model_subchar12367_bert.bin'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["conf_info.bert"]},{"cell_type":"code","execution_count":null,"id":"25982eb9","metadata":{"id":"25982eb9"},"outputs":[],"source":["config = BertConfig(conf_info.bert)"]},{"cell_type":"code","execution_count":null,"id":"8048191a","metadata":{"id":"8048191a"},"outputs":[],"source":["config.vocab_size = 12367"]},{"cell_type":"code","execution_count":null,"id":"9d42be61","metadata":{"id":"9d42be61"},"outputs":[],"source":["model = BertSts(config = config)"]},{"cell_type":"code","execution_count":null,"id":"783c83f4","metadata":{"id":"783c83f4"},"outputs":[],"source":["weights = torch.load(conf_info.bert)"]},{"cell_type":"code","execution_count":null,"id":"b5d2cd7b","metadata":{"id":"b5d2cd7b"},"outputs":[],"source":["param_names = []\n","\n","for name, param in model.named_parameters():\n","    param_names.append(name)"]},{"cell_type":"code","execution_count":null,"id":"b8163c3d","metadata":{"id":"b8163c3d"},"outputs":[],"source":["weight_dict = deepcopy(model.state_dict())"]},{"cell_type":"code","execution_count":null,"id":"cf3190b7","metadata":{"id":"cf3190b7"},"outputs":[],"source":["for name, weight in weights.items():\n","    if name in param_names:\n","        weight_dict[name] = weight"]},{"cell_type":"code","execution_count":null,"id":"5bba4bed","metadata":{"id":"5bba4bed","outputId":"9e5053f2-dde4-4e52-e674-51c5c161e969"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(weight_dict)"]},{"cell_type":"code","execution_count":null,"id":"8356bed7","metadata":{"id":"8356bed7"},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"markdown","id":"b1de2070","metadata":{"id":"b1de2070"},"source":["### 데이터셋 로더 생성"]},{"cell_type":"code","execution_count":null,"id":"226193bc","metadata":{"id":"226193bc"},"outputs":[],"source":["train_dataset = CustomDataset(train.iloc[:, :2].values.tolist(), train['label'].tolist())\n","valid_dataset = CustomDataset(valid.iloc[:, :2].values.tolist(), valid['label'].tolist())"]},{"cell_type":"code","execution_count":null,"id":"2af962cf","metadata":{"id":"2af962cf"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset,\n","                              batch_size = batch_size,\n","                              sampler = RandomSampler(train_dataset),\n","                              collate_fn = custom_collate_fn)\n","\n","valid_dataloader = DataLoader(valid_dataset,\n","                              batch_size = batch_size,\n","                              sampler = RandomSampler(valid_dataset),\n","                              collate_fn = custom_collate_fn)"]},{"cell_type":"markdown","id":"d8151504","metadata":{"id":"d8151504"},"source":["---"]},{"cell_type":"markdown","id":"e5087b92","metadata":{"id":"e5087b92"},"source":["# 4. 모델 학습"]},{"cell_type":"code","execution_count":null,"id":"ab461bb7","metadata":{"id":"ab461bb7"},"outputs":[],"source":["loss_fct = torch.nn.MSELoss()\n","\n","def train(model, train_dataloader, valid_dataloader=None, epochs=2):\n","        global loss_fct, scheduler\n","        digit = len(str(len(train_dataloader)))\n","        early_stopping = EarlyStopping(patience = 5)\n","        \n","        for epoch in range(epochs):\n","            print(f\"*****Epoch {epoch} Train Start*****\")\n","            \n","            total_loss, batch_loss, batch_count = 0,0,0\n","        \n","            model.train()\n","            model.to(device)\n","            \n","            for step, batch in enumerate(train_dataloader):\n","                batch_count+=1\n","                \n","                batch = tuple(item.to(device) for item in batch)\n","            \n","                batch_input, batch_label = batch\n","                \n","                model.zero_grad()\n","            \n","                logits = model(**batch_input)\n","\n","                loss = loss_fct(logits.view(-1), batch_label.view(-1))\n","                \n","                batch_loss += loss.item()\n","                total_loss += loss.item()\n","            \n","                loss.backward()\n","                \n","                clip_grad_norm_(model.parameters(), 1.0)\n","                \n","                optimizer.step()\n","                scheduler.step()\n","                \n","                if (step % 10 == 0 and step != 0):\n","                    learning_rate = optimizer.param_groups[0]['lr']\n","                    print(f\"Epoch: {epoch}, Step: {step:{digit}d}, LR: {learning_rate:.2e}, Avg Loss: {batch_loss / batch_count:.4f}\")\n","\n","                    batch_loss, batch_count = 0,0\n","\n","            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n","            print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n","            \n","            if valid_dataloader:\n","                print(f\"*****Epoch {epoch} Valid Start*****\")\n","                valid_loss = validate(model, valid_dataloader)\n","                print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f}\")\n","                print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n","            \n","\n","            early_stopping(valid_loss)\n","\n","            if early_stopping.early_stop:\n","                print('terminating because of early stopping.')\n","                break\n","                \n","        print(\"Train Completed. End Program.\")"]},{"cell_type":"code","execution_count":null,"id":"58051576","metadata":{"id":"58051576"},"outputs":[],"source":["def validate(model, valid_dataloader):\n","\n","    model.eval()\n","    model.to(device)\n","    \n","    total_loss = 0\n","        \n","    for step, batch in enumerate(valid_dataloader):\n","        batch = tuple(item.to(device) for item in batch)\n","            \n","        batch_input, batch_label = batch\n","\n","        with torch.no_grad():\n","            logits = model(**batch_input)\n","            \n","        loss = loss_fct(logits.view(-1), batch_label.view(-1))\n","        total_loss += loss.item()\n","        \n","    total_loss = total_loss/(step+1)\n","\n","    return total_loss"]},{"cell_type":"code","source":["class EarlyStopping:\n","    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n","    def __init__(self, patience=5, verbose=False, delta=0.0001):\n","        \"\"\"\n","        Args:\n","            patience (int): validation loss가 개선된 후 기다리는 기간\n","                            Default: 7\n","            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n","                            Default: False\n","            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","\n","    def __call__(self, val_loss):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.counter = 0\n","        \n","        if self.verbose and self.val_loss_min > val_loss:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n","            self.val_loss_min = val_loss"],"metadata":{"id":"vNBAlWHJa-TM"},"id":"vNBAlWHJa-TM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ed191fab","metadata":{"id":"ed191fab"},"outputs":[],"source":["def initializer(train_dataloader, epochs=2):\n","    \"\"\"\n","    모델, 옵티마이저, 스케쥴러 초기화\n","    \"\"\"\n","\n","    optimizer = NAdam(\n","        model.parameters(),\n","        lr=2e-5,\n","        eps=1e-8\n","    )\n","    \n","    total_steps = len(train_dataloader) * epochs\n","    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n","\n","    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n","        optimizer, \n","        num_warmup_steps = 0,\n","        num_training_steps = total_steps\n","    )\n","\n","    return optimizer, scheduler"]},{"cell_type":"code","execution_count":null,"id":"fb1b40a4","metadata":{"scrolled":true,"id":"fb1b40a4","outputId":"f03b8095-dc09-4f1c-9521-a170117da6b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total train steps with 30 epochs: 8760\n","*****Epoch 0 Train Start*****\n","Epoch: 0, Step:  10, LR: 2.00e-05, Avg Loss: 5.3951\n","Epoch: 0, Step:  20, LR: 2.00e-05, Avg Loss: 2.4244\n","Epoch: 0, Step:  30, LR: 2.00e-05, Avg Loss: 0.9192\n","Epoch: 0, Step:  40, LR: 2.00e-05, Avg Loss: 0.5671\n","Epoch: 0, Step:  50, LR: 2.00e-05, Avg Loss: 0.5098\n","Epoch: 0, Step:  60, LR: 2.00e-05, Avg Loss: 0.3683\n","Epoch: 0, Step:  70, LR: 2.00e-05, Avg Loss: 0.4303\n","Epoch: 0, Step:  80, LR: 2.00e-05, Avg Loss: 0.3909\n","Epoch: 0, Step:  90, LR: 2.00e-05, Avg Loss: 0.3958\n","Epoch: 0, Step: 100, LR: 2.00e-05, Avg Loss: 0.3448\n","Epoch: 0, Step: 110, LR: 2.00e-05, Avg Loss: 0.3736\n","Epoch: 0, Step: 120, LR: 2.00e-05, Avg Loss: 0.3891\n","Epoch: 0, Step: 130, LR: 2.00e-05, Avg Loss: 0.2574\n","Epoch: 0, Step: 140, LR: 2.00e-05, Avg Loss: 0.2906\n","Epoch: 0, Step: 150, LR: 2.00e-05, Avg Loss: 0.3446\n","Epoch: 0, Step: 160, LR: 2.00e-05, Avg Loss: 0.2786\n","Epoch: 0, Step: 170, LR: 2.00e-05, Avg Loss: 0.2194\n","Epoch: 0, Step: 180, LR: 2.00e-05, Avg Loss: 0.3219\n","Epoch: 0, Step: 190, LR: 2.00e-05, Avg Loss: 0.2434\n","Epoch: 0, Step: 200, LR: 2.00e-05, Avg Loss: 0.3391\n","Epoch: 0, Step: 210, LR: 2.00e-05, Avg Loss: 0.3870\n","Epoch: 0, Step: 220, LR: 2.00e-05, Avg Loss: 0.3001\n","Epoch: 0, Step: 230, LR: 2.00e-05, Avg Loss: 0.2679\n","Epoch: 0, Step: 240, LR: 2.00e-05, Avg Loss: 0.2127\n","Epoch: 0, Step: 250, LR: 2.00e-05, Avg Loss: 0.1965\n","Epoch: 0, Step: 260, LR: 2.00e-05, Avg Loss: 0.3042\n","Epoch: 0, Step: 270, LR: 2.00e-05, Avg Loss: 0.2935\n","Epoch: 0, Step: 280, LR: 1.99e-05, Avg Loss: 0.2344\n","Epoch: 0, Step: 290, LR: 1.99e-05, Avg Loss: 0.2531\n","Epoch 0 Total Mean Loss : 0.6101\n","*****Epoch 0 Train Finish*****\n","\n","*****Epoch 0 Valid Start*****\n","Epoch 0 Valid Loss : 0.2301\n","*****Epoch 0 Valid Finish*****\n","\n","*****Epoch 1 Train Start*****\n","Epoch: 1, Step:  10, LR: 1.99e-05, Avg Loss: 0.1579\n","Epoch: 1, Step:  20, LR: 1.99e-05, Avg Loss: 0.1873\n","Epoch: 1, Step:  30, LR: 1.99e-05, Avg Loss: 0.1661\n","Epoch: 1, Step:  40, LR: 1.99e-05, Avg Loss: 0.2131\n","Epoch: 1, Step:  50, LR: 1.99e-05, Avg Loss: 0.1856\n","Epoch: 1, Step:  60, LR: 1.99e-05, Avg Loss: 0.1570\n","Epoch: 1, Step:  70, LR: 1.99e-05, Avg Loss: 0.1747\n","Epoch: 1, Step:  80, LR: 1.99e-05, Avg Loss: 0.1608\n","Epoch: 1, Step:  90, LR: 1.99e-05, Avg Loss: 0.2125\n","Epoch: 1, Step: 100, LR: 1.99e-05, Avg Loss: 0.1946\n","Epoch: 1, Step: 110, LR: 1.99e-05, Avg Loss: 0.1991\n","Epoch: 1, Step: 120, LR: 1.99e-05, Avg Loss: 0.2015\n","Epoch: 1, Step: 130, LR: 1.99e-05, Avg Loss: 0.1325\n","Epoch: 1, Step: 140, LR: 1.99e-05, Avg Loss: 0.1721\n","Epoch: 1, Step: 150, LR: 1.99e-05, Avg Loss: 0.1959\n","Epoch: 1, Step: 160, LR: 1.99e-05, Avg Loss: 0.2073\n","Epoch: 1, Step: 170, LR: 1.99e-05, Avg Loss: 0.2206\n","Epoch: 1, Step: 180, LR: 1.99e-05, Avg Loss: 0.1856\n","Epoch: 1, Step: 190, LR: 1.99e-05, Avg Loss: 0.1851\n","Epoch: 1, Step: 200, LR: 1.98e-05, Avg Loss: 0.1409\n","Epoch: 1, Step: 210, LR: 1.98e-05, Avg Loss: 0.2027\n","Epoch: 1, Step: 220, LR: 1.98e-05, Avg Loss: 0.1774\n","Epoch: 1, Step: 230, LR: 1.98e-05, Avg Loss: 0.1617\n","Epoch: 1, Step: 240, LR: 1.98e-05, Avg Loss: 0.1908\n","Epoch: 1, Step: 250, LR: 1.98e-05, Avg Loss: 0.2357\n","Epoch: 1, Step: 260, LR: 1.98e-05, Avg Loss: 0.1533\n","Epoch: 1, Step: 270, LR: 1.98e-05, Avg Loss: 0.1426\n","Epoch: 1, Step: 280, LR: 1.98e-05, Avg Loss: 0.1694\n","Epoch: 1, Step: 290, LR: 1.98e-05, Avg Loss: 0.1688\n","Epoch 1 Total Mean Loss : 0.1808\n","*****Epoch 1 Train Finish*****\n","\n","*****Epoch 1 Valid Start*****\n","Epoch 1 Valid Loss : 0.2234\n","*****Epoch 1 Valid Finish*****\n","\n","*****Epoch 2 Train Start*****\n","Epoch: 2, Step:  10, LR: 1.98e-05, Avg Loss: 0.1242\n","Epoch: 2, Step:  20, LR: 1.98e-05, Avg Loss: 0.1432\n","Epoch: 2, Step:  30, LR: 1.98e-05, Avg Loss: 0.1082\n","Epoch: 2, Step:  40, LR: 1.97e-05, Avg Loss: 0.1135\n","Epoch: 2, Step:  50, LR: 1.97e-05, Avg Loss: 0.1090\n","Epoch: 2, Step:  60, LR: 1.97e-05, Avg Loss: 0.1158\n","Epoch: 2, Step:  70, LR: 1.97e-05, Avg Loss: 0.1553\n","Epoch: 2, Step:  80, LR: 1.97e-05, Avg Loss: 0.1362\n","Epoch: 2, Step:  90, LR: 1.97e-05, Avg Loss: 0.1234\n","Epoch: 2, Step: 100, LR: 1.97e-05, Avg Loss: 0.1239\n","Epoch: 2, Step: 110, LR: 1.97e-05, Avg Loss: 0.1271\n","Epoch: 2, Step: 120, LR: 1.97e-05, Avg Loss: 0.1363\n","Epoch: 2, Step: 130, LR: 1.97e-05, Avg Loss: 0.1351\n","Epoch: 2, Step: 140, LR: 1.97e-05, Avg Loss: 0.1213\n","Epoch: 2, Step: 150, LR: 1.97e-05, Avg Loss: 0.1234\n","Epoch: 2, Step: 160, LR: 1.96e-05, Avg Loss: 0.1097\n","Epoch: 2, Step: 170, LR: 1.96e-05, Avg Loss: 0.1233\n","Epoch: 2, Step: 180, LR: 1.96e-05, Avg Loss: 0.1016\n","Epoch: 2, Step: 190, LR: 1.96e-05, Avg Loss: 0.1257\n","Epoch: 2, Step: 200, LR: 1.96e-05, Avg Loss: 0.1241\n","Epoch: 2, Step: 210, LR: 1.96e-05, Avg Loss: 0.1202\n","Epoch: 2, Step: 220, LR: 1.96e-05, Avg Loss: 0.0953\n","Epoch: 2, Step: 230, LR: 1.96e-05, Avg Loss: 0.1246\n","Epoch: 2, Step: 240, LR: 1.96e-05, Avg Loss: 0.1133\n","Epoch: 2, Step: 250, LR: 1.96e-05, Avg Loss: 0.1108\n","Epoch: 2, Step: 260, LR: 1.95e-05, Avg Loss: 0.1143\n","Epoch: 2, Step: 270, LR: 1.95e-05, Avg Loss: 0.1356\n","Epoch: 2, Step: 280, LR: 1.95e-05, Avg Loss: 0.1136\n","Epoch: 2, Step: 290, LR: 1.95e-05, Avg Loss: 0.1190\n","Epoch 2 Total Mean Loss : 0.1216\n","*****Epoch 2 Train Finish*****\n","\n","*****Epoch 2 Valid Start*****\n","Epoch 2 Valid Loss : 0.1866\n","*****Epoch 2 Valid Finish*****\n","\n","*****Epoch 3 Train Start*****\n","Epoch: 3, Step:  10, LR: 1.95e-05, Avg Loss: 0.0971\n","Epoch: 3, Step:  20, LR: 1.95e-05, Avg Loss: 0.1000\n","Epoch: 3, Step:  30, LR: 1.95e-05, Avg Loss: 0.0970\n","Epoch: 3, Step:  40, LR: 1.95e-05, Avg Loss: 0.1002\n","Epoch: 3, Step:  50, LR: 1.95e-05, Avg Loss: 0.0853\n","Epoch: 3, Step:  60, LR: 1.94e-05, Avg Loss: 0.0989\n","Epoch: 3, Step:  70, LR: 1.94e-05, Avg Loss: 0.0824\n","Epoch: 3, Step:  80, LR: 1.94e-05, Avg Loss: 0.0853\n","Epoch: 3, Step:  90, LR: 1.94e-05, Avg Loss: 0.1045\n","Epoch: 3, Step: 100, LR: 1.94e-05, Avg Loss: 0.0870\n","Epoch: 3, Step: 110, LR: 1.94e-05, Avg Loss: 0.0835\n","Epoch: 3, Step: 120, LR: 1.94e-05, Avg Loss: 0.0782\n","Epoch: 3, Step: 130, LR: 1.94e-05, Avg Loss: 0.0799\n","Epoch: 3, Step: 140, LR: 1.93e-05, Avg Loss: 0.0934\n","Epoch: 3, Step: 150, LR: 1.93e-05, Avg Loss: 0.1054\n","Epoch: 3, Step: 160, LR: 1.93e-05, Avg Loss: 0.0722\n","Epoch: 3, Step: 170, LR: 1.93e-05, Avg Loss: 0.1113\n","Epoch: 3, Step: 180, LR: 1.93e-05, Avg Loss: 0.0824\n","Epoch: 3, Step: 190, LR: 1.93e-05, Avg Loss: 0.1002\n","Epoch: 3, Step: 200, LR: 1.93e-05, Avg Loss: 0.0813\n","Epoch: 3, Step: 210, LR: 1.92e-05, Avg Loss: 0.0989\n","Epoch: 3, Step: 220, LR: 1.92e-05, Avg Loss: 0.0831\n","Epoch: 3, Step: 230, LR: 1.92e-05, Avg Loss: 0.0865\n","Epoch: 3, Step: 240, LR: 1.92e-05, Avg Loss: 0.0879\n","Epoch: 3, Step: 250, LR: 1.92e-05, Avg Loss: 0.0776\n","Epoch: 3, Step: 260, LR: 1.92e-05, Avg Loss: 0.0800\n","Epoch: 3, Step: 270, LR: 1.92e-05, Avg Loss: 0.0903\n","Epoch: 3, Step: 280, LR: 1.92e-05, Avg Loss: 0.1066\n","Epoch: 3, Step: 290, LR: 1.91e-05, Avg Loss: 0.1020\n","Epoch 3 Total Mean Loss : 0.0909\n","*****Epoch 3 Train Finish*****\n","\n","*****Epoch 3 Valid Start*****\n","Epoch 3 Valid Loss : 0.2068\n","*****Epoch 3 Valid Finish*****\n","\n","EarlyStopping counter: 1 out of 5\n","*****Epoch 4 Train Start*****\n","Epoch: 4, Step:  10, LR: 1.91e-05, Avg Loss: 0.0684\n","Epoch: 4, Step:  20, LR: 1.91e-05, Avg Loss: 0.0682\n","Epoch: 4, Step:  30, LR: 1.91e-05, Avg Loss: 0.0778\n","Epoch: 4, Step:  40, LR: 1.91e-05, Avg Loss: 0.0708\n","Epoch: 4, Step:  50, LR: 1.91e-05, Avg Loss: 0.0639\n","Epoch: 4, Step:  60, LR: 1.90e-05, Avg Loss: 0.0624\n","Epoch: 4, Step:  70, LR: 1.90e-05, Avg Loss: 0.0757\n","Epoch: 4, Step:  80, LR: 1.90e-05, Avg Loss: 0.0795\n","Epoch: 4, Step:  90, LR: 1.90e-05, Avg Loss: 0.0790\n","Epoch: 4, Step: 100, LR: 1.90e-05, Avg Loss: 0.0658\n","Epoch: 4, Step: 110, LR: 1.90e-05, Avg Loss: 0.0627\n","Epoch: 4, Step: 120, LR: 1.90e-05, Avg Loss: 0.0648\n","Epoch: 4, Step: 130, LR: 1.89e-05, Avg Loss: 0.0593\n","Epoch: 4, Step: 140, LR: 1.89e-05, Avg Loss: 0.0597\n","Epoch: 4, Step: 150, LR: 1.89e-05, Avg Loss: 0.0714\n","Epoch: 4, Step: 160, LR: 1.89e-05, Avg Loss: 0.0850\n","Epoch: 4, Step: 170, LR: 1.89e-05, Avg Loss: 0.0607\n","Epoch: 4, Step: 180, LR: 1.89e-05, Avg Loss: 0.0722\n","Epoch: 4, Step: 190, LR: 1.88e-05, Avg Loss: 0.0704\n","Epoch: 4, Step: 200, LR: 1.88e-05, Avg Loss: 0.0753\n","Epoch: 4, Step: 210, LR: 1.88e-05, Avg Loss: 0.0712\n","Epoch: 4, Step: 220, LR: 1.88e-05, Avg Loss: 0.0562\n","Epoch: 4, Step: 230, LR: 1.88e-05, Avg Loss: 0.0698\n","Epoch: 4, Step: 240, LR: 1.88e-05, Avg Loss: 0.0714\n","Epoch: 4, Step: 250, LR: 1.87e-05, Avg Loss: 0.0761\n","Epoch: 4, Step: 260, LR: 1.87e-05, Avg Loss: 0.0604\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4, Step: 270, LR: 1.87e-05, Avg Loss: 0.0725\n","Epoch: 4, Step: 280, LR: 1.87e-05, Avg Loss: 0.0681\n","Epoch: 4, Step: 290, LR: 1.87e-05, Avg Loss: 0.0776\n","Epoch 4 Total Mean Loss : 0.0696\n","*****Epoch 4 Train Finish*****\n","\n","*****Epoch 4 Valid Start*****\n","Epoch 4 Valid Loss : 0.2231\n","*****Epoch 4 Valid Finish*****\n","\n","EarlyStopping counter: 2 out of 5\n","*****Epoch 5 Train Start*****\n","Epoch: 5, Step:  10, LR: 1.86e-05, Avg Loss: 0.0573\n","Epoch: 5, Step:  20, LR: 1.86e-05, Avg Loss: 0.0544\n","Epoch: 5, Step:  30, LR: 1.86e-05, Avg Loss: 0.0593\n","Epoch: 5, Step:  40, LR: 1.86e-05, Avg Loss: 0.0523\n","Epoch: 5, Step:  50, LR: 1.86e-05, Avg Loss: 0.0590\n","Epoch: 5, Step:  60, LR: 1.85e-05, Avg Loss: 0.0513\n","Epoch: 5, Step:  70, LR: 1.85e-05, Avg Loss: 0.0592\n","Epoch: 5, Step:  80, LR: 1.85e-05, Avg Loss: 0.0505\n","Epoch: 5, Step:  90, LR: 1.85e-05, Avg Loss: 0.0670\n","Epoch: 5, Step: 100, LR: 1.85e-05, Avg Loss: 0.0465\n","Epoch: 5, Step: 110, LR: 1.85e-05, Avg Loss: 0.0516\n","Epoch: 5, Step: 120, LR: 1.84e-05, Avg Loss: 0.0571\n","Epoch: 5, Step: 130, LR: 1.84e-05, Avg Loss: 0.0553\n","Epoch: 5, Step: 140, LR: 1.84e-05, Avg Loss: 0.0676\n","Epoch: 5, Step: 150, LR: 1.84e-05, Avg Loss: 0.0844\n","Epoch: 5, Step: 160, LR: 1.84e-05, Avg Loss: 0.0572\n","Epoch: 5, Step: 170, LR: 1.83e-05, Avg Loss: 0.0501\n","Epoch: 5, Step: 180, LR: 1.83e-05, Avg Loss: 0.0521\n","Epoch: 5, Step: 190, LR: 1.83e-05, Avg Loss: 0.0636\n","Epoch: 5, Step: 200, LR: 1.83e-05, Avg Loss: 0.0567\n","Epoch: 5, Step: 210, LR: 1.83e-05, Avg Loss: 0.0629\n","Epoch: 5, Step: 220, LR: 1.82e-05, Avg Loss: 0.0462\n","Epoch: 5, Step: 230, LR: 1.82e-05, Avg Loss: 0.0496\n","Epoch: 5, Step: 240, LR: 1.82e-05, Avg Loss: 0.0512\n","Epoch: 5, Step: 250, LR: 1.82e-05, Avg Loss: 0.0488\n","Epoch: 5, Step: 260, LR: 1.82e-05, Avg Loss: 0.0580\n","Epoch: 5, Step: 270, LR: 1.81e-05, Avg Loss: 0.0479\n","Epoch: 5, Step: 280, LR: 1.81e-05, Avg Loss: 0.0631\n","Epoch: 5, Step: 290, LR: 1.81e-05, Avg Loss: 0.0593\n","Epoch 5 Total Mean Loss : 0.0566\n","*****Epoch 5 Train Finish*****\n","\n","*****Epoch 5 Valid Start*****\n","Epoch 5 Valid Loss : 0.2153\n","*****Epoch 5 Valid Finish*****\n","\n","EarlyStopping counter: 3 out of 5\n","*****Epoch 6 Train Start*****\n","Epoch: 6, Step:  10, LR: 1.81e-05, Avg Loss: 0.0507\n","Epoch: 6, Step:  20, LR: 1.80e-05, Avg Loss: 0.0408\n","Epoch: 6, Step:  30, LR: 1.80e-05, Avg Loss: 0.0405\n","Epoch: 6, Step:  40, LR: 1.80e-05, Avg Loss: 0.0443\n","Epoch: 6, Step:  50, LR: 1.80e-05, Avg Loss: 0.0508\n","Epoch: 6, Step:  60, LR: 1.80e-05, Avg Loss: 0.0519\n","Epoch: 6, Step:  70, LR: 1.79e-05, Avg Loss: 0.0425\n","Epoch: 6, Step:  80, LR: 1.79e-05, Avg Loss: 0.0556\n","Epoch: 6, Step:  90, LR: 1.79e-05, Avg Loss: 0.0476\n","Epoch: 6, Step: 100, LR: 1.79e-05, Avg Loss: 0.0448\n","Epoch: 6, Step: 110, LR: 1.78e-05, Avg Loss: 0.0525\n","Epoch: 6, Step: 120, LR: 1.78e-05, Avg Loss: 0.0477\n","Epoch: 6, Step: 130, LR: 1.78e-05, Avg Loss: 0.0486\n","Epoch: 6, Step: 140, LR: 1.78e-05, Avg Loss: 0.0502\n","Epoch: 6, Step: 150, LR: 1.78e-05, Avg Loss: 0.0398\n","Epoch: 6, Step: 160, LR: 1.77e-05, Avg Loss: 0.0478\n","Epoch: 6, Step: 170, LR: 1.77e-05, Avg Loss: 0.0539\n","Epoch: 6, Step: 180, LR: 1.77e-05, Avg Loss: 0.0464\n","Epoch: 6, Step: 190, LR: 1.77e-05, Avg Loss: 0.0395\n","Epoch: 6, Step: 200, LR: 1.76e-05, Avg Loss: 0.0484\n","Epoch: 6, Step: 210, LR: 1.76e-05, Avg Loss: 0.0408\n","Epoch: 6, Step: 220, LR: 1.76e-05, Avg Loss: 0.0551\n","Epoch: 6, Step: 230, LR: 1.76e-05, Avg Loss: 0.0363\n","Epoch: 6, Step: 240, LR: 1.76e-05, Avg Loss: 0.0589\n","Epoch: 6, Step: 250, LR: 1.75e-05, Avg Loss: 0.0371\n","Epoch: 6, Step: 260, LR: 1.75e-05, Avg Loss: 0.0423\n","Epoch: 6, Step: 270, LR: 1.75e-05, Avg Loss: 0.0561\n","Epoch: 6, Step: 280, LR: 1.75e-05, Avg Loss: 0.0387\n","Epoch: 6, Step: 290, LR: 1.74e-05, Avg Loss: 0.0507\n","Epoch 6 Total Mean Loss : 0.0469\n","*****Epoch 6 Train Finish*****\n","\n","*****Epoch 6 Valid Start*****\n","Epoch 6 Valid Loss : 0.1958\n","*****Epoch 6 Valid Finish*****\n","\n","EarlyStopping counter: 4 out of 5\n","*****Epoch 7 Train Start*****\n","Epoch: 7, Step:  10, LR: 1.74e-05, Avg Loss: 0.0373\n","Epoch: 7, Step:  20, LR: 1.74e-05, Avg Loss: 0.0398\n","Epoch: 7, Step:  30, LR: 1.74e-05, Avg Loss: 0.0432\n","Epoch: 7, Step:  40, LR: 1.73e-05, Avg Loss: 0.0418\n","Epoch: 7, Step:  50, LR: 1.73e-05, Avg Loss: 0.0356\n","Epoch: 7, Step:  60, LR: 1.73e-05, Avg Loss: 0.0427\n","Epoch: 7, Step:  70, LR: 1.73e-05, Avg Loss: 0.0331\n","Epoch: 7, Step:  80, LR: 1.72e-05, Avg Loss: 0.0397\n","Epoch: 7, Step:  90, LR: 1.72e-05, Avg Loss: 0.0373\n","Epoch: 7, Step: 100, LR: 1.72e-05, Avg Loss: 0.0353\n","Epoch: 7, Step: 110, LR: 1.72e-05, Avg Loss: 0.0379\n","Epoch: 7, Step: 120, LR: 1.71e-05, Avg Loss: 0.0413\n","Epoch: 7, Step: 130, LR: 1.71e-05, Avg Loss: 0.0392\n","Epoch: 7, Step: 140, LR: 1.71e-05, Avg Loss: 0.0372\n","Epoch: 7, Step: 150, LR: 1.71e-05, Avg Loss: 0.0356\n","Epoch: 7, Step: 160, LR: 1.70e-05, Avg Loss: 0.0386\n","Epoch: 7, Step: 170, LR: 1.70e-05, Avg Loss: 0.0390\n","Epoch: 7, Step: 180, LR: 1.70e-05, Avg Loss: 0.0401\n","Epoch: 7, Step: 190, LR: 1.70e-05, Avg Loss: 0.0404\n","Epoch: 7, Step: 200, LR: 1.69e-05, Avg Loss: 0.0417\n","Epoch: 7, Step: 210, LR: 1.69e-05, Avg Loss: 0.0418\n","Epoch: 7, Step: 220, LR: 1.69e-05, Avg Loss: 0.0409\n","Epoch: 7, Step: 230, LR: 1.69e-05, Avg Loss: 0.0387\n","Epoch: 7, Step: 240, LR: 1.68e-05, Avg Loss: 0.0362\n","Epoch: 7, Step: 250, LR: 1.68e-05, Avg Loss: 0.0364\n","Epoch: 7, Step: 260, LR: 1.68e-05, Avg Loss: 0.0413\n","Epoch: 7, Step: 270, LR: 1.67e-05, Avg Loss: 0.0361\n","Epoch: 7, Step: 280, LR: 1.67e-05, Avg Loss: 0.0383\n","Epoch: 7, Step: 290, LR: 1.67e-05, Avg Loss: 0.0464\n","Epoch 7 Total Mean Loss : 0.0391\n","*****Epoch 7 Train Finish*****\n","\n","*****Epoch 7 Valid Start*****\n","Epoch 7 Valid Loss : 0.2127\n","*****Epoch 7 Valid Finish*****\n","\n","EarlyStopping counter: 5 out of 5\n","terminating because of early stopping.\n","Train Completed. End Program.\n"]}],"source":["epochs = 30\n","optimizer, scheduler = initializer(train_dataloader, epochs = epochs)\n","\n","train(model, train_dataloader, valid_dataloader, epochs = epochs)"]},{"cell_type":"markdown","id":"9181bffc","metadata":{"id":"9181bffc"},"source":["# make output"]},{"cell_type":"code","execution_count":null,"id":"8999ed74","metadata":{"id":"8999ed74"},"outputs":[],"source":["def predict(model, test_dataloader):\n","    \"\"\"\n","    test_dataloader의 label별 확률값과 실제 label 값을 반환\n","    \"\"\"\n","\n","    model.eval()\n","    model.to(device)\n","\n","    all_logits = []\n","    all_labels = []\n","\n","    for step, batch in enumerate(test_dataloader):\n","        print(f\"{step+1}/{len(test_dataloader)}\\r\", end = \"\")\n","        \n","        batch_input, batch_label = batch\n","        \n","        batch_input = batch_input.to(device)\n","        \n","        with torch.no_grad():\n","            logits = model(**batch_input)\n","            all_logits.append(logits)\n","        all_labels.extend(batch_label)\n","\n","    all_logits = torch.cat(all_logits, dim=0)\n","    probs = torch.tensor(all_logits).cpu().numpy()\n","    all_labels = np.array(all_labels)\n","\n","    return probs, all_labels"]},{"cell_type":"markdown","id":"26ddcfd4","metadata":{"id":"26ddcfd4"},"source":["# validationset score"]},{"cell_type":"code","execution_count":null,"id":"2af07f94","metadata":{"id":"2af07f94","outputId":"f1626d58-fb2e-4c60-cdf6-f0d79e653d10"},"outputs":[{"name":"stdout","output_type":"stream","text":["73/73\r"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\HwaLang\\AppData\\Local\\Temp/ipykernel_21488/3166901217.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  probs = torch.tensor(all_logits).cpu().numpy()\n"]}],"source":["val_probs, val_labels = predict(model, valid_dataloader)"]},{"cell_type":"code","execution_count":null,"id":"a36e439c","metadata":{"id":"a36e439c","outputId":"80b7c631-d67a-4519-f98c-f6c0d51dfea8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pearson r: 0.97 \n","P-value: 0.00e+00\n"]}],"source":["print(\"Pearson r: {:.2f} \\nP-value: {:.2e}\".format(*pearsonr(val_probs.flatten(), val_labels)))"]},{"cell_type":"code","execution_count":null,"id":"51946c31","metadata":{"id":"51946c31","outputId":"08105e1e-ad7d-4ca3-e6fe-082929ccc119"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 score: 0.9551227773073666\n"]}],"source":["print(\"F1 score:\", f1_score(np.where(val_probs.flatten() >= 3, 1, 0), np.where(val_labels >= 3, 1, 0)))"]},{"cell_type":"code","execution_count":null,"id":"2692355d","metadata":{"id":"2692355d"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"4f096bc2","metadata":{"id":"4f096bc2"},"source":["# devset score"]},{"cell_type":"code","source":["with open(\"./klue-sts-data/klue-sts-v1.1_dev.json\", \"rt\", encoding='utf8') as f:\n","    dev_data = json.load(f)"],"metadata":{"id":"97HJKrHUbHRJ"},"id":"97HJKrHUbHRJ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9a0ab64c","metadata":{"id":"9a0ab64c"},"outputs":[],"source":["shape = np.full([len(dev_data), 3], np.nan)\n","dev_df = pd.DataFrame(shape, columns=['sentence1', 'sentence2', 'label'])\n","\n","for idx, el in enumerate(dev_data):\n","    dev_df.loc[idx] = [el['sentence1'], el['sentence2'], el['labels']['real-label']]\n","\n","dev_df[['sentence1', 'sentence2']] = dev_df[['sentence1', 'sentence2']].applymap(data_preproc)"]},{"cell_type":"code","execution_count":null,"id":"8c1e927a","metadata":{"id":"8c1e927a"},"outputs":[],"source":["dev_dataset = CustomDataset(dev_df.iloc[:, :2].values.tolist(), dev_df['label'].tolist())\n","\n","dev_dataloader = DataLoader(dev_dataset,\n","                            batch_size = batch_size,\n","                            sampler = RandomSampler(dev_dataset),\n","                            collate_fn = custom_collate_fn)"]},{"cell_type":"code","execution_count":null,"id":"684d8005","metadata":{"scrolled":false,"id":"684d8005","outputId":"26249a3e-cd3d-4a60-bf64-98f852e55f26"},"outputs":[{"name":"stdout","output_type":"stream","text":["17/17\r"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\HwaLang\\AppData\\Local\\Temp/ipykernel_21488/3166901217.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  probs = torch.tensor(all_logits).cpu().numpy()\n"]}],"source":["probs, labels = predict(model, dev_dataloader)"]},{"cell_type":"code","execution_count":null,"id":"e3ee190f","metadata":{"scrolled":false,"id":"e3ee190f","outputId":"68111d41-a310-4a12-c855-6f02a0a9ac97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pearson r: 0.86 \n","P-value: 3.22e-154\n"]}],"source":["print(\"Pearson r: {:.2f} \\nP-value: {:.2e}\".format(*pearsonr(probs.flatten(), labels)))"]},{"cell_type":"code","execution_count":null,"id":"84b8285a","metadata":{"scrolled":false,"id":"84b8285a","outputId":"27b3141b-2cc3-4f0d-bba6-e313e5239d10"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 score: 0.7903225806451614\n"]}],"source":["print(\"F1 score:\", f1_score(np.where(probs.flatten() >= 3, 1, 0), np.where(labels >= 3, 1, 0)))"]},{"cell_type":"code","execution_count":null,"id":"120ff33d","metadata":{"id":"120ff33d"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"기업과제3_4팀.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}